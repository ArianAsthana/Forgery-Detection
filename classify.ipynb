{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3bc0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset as TorchDataset, DataLoader\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import easyocr\n",
    "import gc\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "import json\n",
    "import tempfile\n",
    "\n",
    "# Check transformers version and import appropriate LayoutLM model\n",
    "try:\n",
    "    import transformers\n",
    "    print(f\"Transformers version: {transformers.__version__}\")\n",
    "    \n",
    "    # Try to import LayoutLMv3 first (requires transformers >= 4.21.0)\n",
    "    try:\n",
    "        from transformers import LayoutLMv3Processor, LayoutLMv3ForSequenceClassification\n",
    "        MODEL_NAME = \"microsoft/layoutlmv3-base\"\n",
    "        PROCESSOR_CLASS = LayoutLMv3Processor\n",
    "        MODEL_CLASS = LayoutLMv3ForSequenceClassification\n",
    "        print(\"‚úÖ Using LayoutLMv3\")\n",
    "    except ImportError:\n",
    "        print(\"‚ùå LayoutLMv3 not available, trying LayoutLMv2...\")\n",
    "        try:\n",
    "            from transformers import LayoutLMv2Processor, LayoutLMv2ForSequenceClassification\n",
    "            MODEL_NAME = \"microsoft/layoutlmv2-base-uncased\"\n",
    "            PROCESSOR_CLASS = LayoutLMv2Processor\n",
    "            MODEL_CLASS = LayoutLMv2ForSequenceClassification\n",
    "            print(\"‚úÖ Using LayoutLMv2\")\n",
    "        except ImportError:\n",
    "            print(\"‚ùå Neither LayoutLMv3 nor LayoutLMv2 available\")\n",
    "            print(\"üìù Please upgrade transformers: pip install transformers>=4.21.0\")\n",
    "            raise ImportError(\"LayoutLM models not available. Please upgrade transformers library.\")\n",
    "            \n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Transformers library not found: {e}\")\n",
    "    print(\"üìù Please install transformers: pip install transformers>=4.21.0\")\n",
    "    raise\n",
    "\n",
    "# ---------------------- CONFIG ----------------------\n",
    "DATA_DIR = \"/home/hasan/datasets/classify/test\"\n",
    "MAX_LENGTH = 128\n",
    "BATCH_SIZE = 2  # Reduced batch size\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 2e-5\n",
    "USE_GPU = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda\" if USE_GPU else \"cpu\")\n",
    "CACHE_DIR = \"./cache\"  # Directory to cache processed samples\n",
    "PROCESS_BATCH_SIZE = 50  # Process samples in smaller batches\n",
    "SAVE_DIR = \"./models\"  # Directory to save trained models\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"üîß Configuration:\")\n",
    "print(f\"   Model: {MODEL_NAME}\")\n",
    "print(f\"   Device: {DEVICE}\")\n",
    "print(f\"   Batch size: {BATCH_SIZE}\")\n",
    "print(f\"   Max length: {MAX_LENGTH}\")\n",
    "print(f\"   Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"   Epochs: {EPOCHS}\")\n",
    "\n",
    "# ---------------------- INIT ----------------------\n",
    "try:\n",
    "    processor = PROCESSOR_CLASS.from_pretrained(MODEL_NAME, apply_ocr=False)\n",
    "    print(\"‚úÖ Processor loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to load processor: {e}\")\n",
    "    raise\n",
    "\n",
    "# Initialize EasyOCR with limited GPU memory\n",
    "try:\n",
    "    reader = easyocr.Reader(['en'], gpu=USE_GPU, model_storage_directory='./easyocr_models')\n",
    "    print(\"‚úÖ EasyOCR initialized successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to initialize EasyOCR: {e}\")\n",
    "    print(\"üìù Please install EasyOCR: pip install easyocr\")\n",
    "    raise\n",
    "\n",
    "# ---------------------- MEMORY MANAGEMENT ----------------------\n",
    "def clear_memory():\n",
    "    \"\"\"Clear GPU and system memory\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "def get_memory_usage():\n",
    "    \"\"\"Get current GPU memory usage\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.cuda.memory_allocated() / 1024**3  # GB\n",
    "    return 0\n",
    "\n",
    "# ---------------------- LOAD DATA ----------------------\n",
    "def load_data(data_dir):\n",
    "    if not os.path.exists(data_dir):\n",
    "        raise FileNotFoundError(f\"Data directory not found: {data_dir}\")\n",
    "    \n",
    "    samples = []\n",
    "    label_map = {}\n",
    "    label_id = 0\n",
    "\n",
    "    for label_name in sorted(os.listdir(data_dir)):\n",
    "        label_path = os.path.join(data_dir, label_name)\n",
    "        if not os.path.isdir(label_path):\n",
    "            continue\n",
    "\n",
    "        if label_name not in label_map:\n",
    "            label_map[label_name] = label_id\n",
    "            label_id += 1\n",
    "\n",
    "        for file in os.listdir(label_path):\n",
    "            if file.lower().endswith((\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tiff\")):\n",
    "                samples.append({\n",
    "                    \"image_path\": os.path.join(label_path, file),\n",
    "                    \"label\": label_map[label_name],\n",
    "                    \"filename\": file\n",
    "                })\n",
    "\n",
    "    print(f\"üìä Dataset Summary:\")\n",
    "    print(f\"   Unique labels: {sorted(label_map.items(), key=lambda x: x[1])}\")\n",
    "    print(f\"   Total samples: {len(samples)}\")\n",
    "    \n",
    "    if len(samples) == 0:\n",
    "        raise ValueError(\"No image files found in the dataset directory\")\n",
    "    \n",
    "    return samples, label_map\n",
    "\n",
    "# ---------------------- ROBUST CACHE MANAGEMENT ----------------------\n",
    "def safe_save_cache(data, filepath):\n",
    "    \"\"\"Safely save data to cache with atomic write\"\"\"\n",
    "    temp_file = None\n",
    "    try:\n",
    "        # Use temporary file for atomic write\n",
    "        temp_dir = os.path.dirname(filepath)\n",
    "        with tempfile.NamedTemporaryFile(dir=temp_dir, delete=False, suffix='.tmp') as f:\n",
    "            temp_file = f.name\n",
    "            pickle.dump(data, f)\n",
    "        \n",
    "        # Atomic move\n",
    "        os.rename(temp_file, filepath)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save cache {filepath}: {e}\")\n",
    "        if temp_file and os.path.exists(temp_file):\n",
    "            os.unlink(temp_file)\n",
    "        return False\n",
    "\n",
    "def safe_load_cache(filepath):\n",
    "    \"\"\"Safely load cache with validation\"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        with open(filepath, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        return data\n",
    "    except (EOFError, pickle.UnpicklingError, Exception) as e:\n",
    "        print(f\"Cache file corrupted: {filepath}, error: {e}\")\n",
    "        print(\"Deleting corrupted cache file...\")\n",
    "        try:\n",
    "            os.unlink(filepath)\n",
    "        except:\n",
    "            pass\n",
    "        return None\n",
    "\n",
    "# ---------------------- PROCESS SAMPLE ----------------------\n",
    "def process_single(sample, max_retries=2):\n",
    "    \"\"\"Process a single sample with error handling and retries\"\"\"\n",
    "    for attempt in range(max_retries + 1):\n",
    "        try:\n",
    "            image_path = sample[\"image_path\"]\n",
    "            \n",
    "            # Load and resize image if too large\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            \n",
    "            # Resize very large images to prevent memory issues\n",
    "            max_size = 2048\n",
    "            if max(image.size) > max_size:\n",
    "                ratio = max_size / max(image.size)\n",
    "                new_size = tuple(int(dim * ratio) for dim in image.size)\n",
    "                image = image.resize(new_size, Image.Resampling.LANCZOS)\n",
    "            \n",
    "            img_np = np.array(image)\n",
    "            \n",
    "            # OCR with error handling\n",
    "            try:\n",
    "                results = reader.readtext(img_np)\n",
    "            except Exception as ocr_error:\n",
    "                print(f\"OCR failed for {sample['filename']}: {ocr_error}\")\n",
    "                results = []\n",
    "            \n",
    "            words, boxes = [], []\n",
    "            \n",
    "            for box, text, conf in results:\n",
    "                if conf > 0.5 and text.strip():\n",
    "                    words.append(text.strip())\n",
    "                    x0 = min(pt[0] for pt in box)\n",
    "                    y0 = min(pt[1] for pt in box)\n",
    "                    x1 = max(pt[0] for pt in box)\n",
    "                    y1 = max(pt[1] for pt in box)\n",
    "                    boxes.append([int(x0), int(y0), int(x1), int(y1)])\n",
    "\n",
    "            # Fallback for images with no text\n",
    "            if not words:\n",
    "                words = [\"[EMPTY]\"]\n",
    "                boxes = [[0, 0, 50, 20]]\n",
    "\n",
    "            # Limit number of words to prevent memory issues\n",
    "            if len(words) > 100:\n",
    "                words = words[:100]\n",
    "                boxes = boxes[:100]\n",
    "\n",
    "            # Process with the appropriate processor\n",
    "            encoded = processor(\n",
    "                images=image,\n",
    "                text=words,\n",
    "                boxes=boxes,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                max_length=MAX_LENGTH,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "\n",
    "            # Process encoded data\n",
    "            result = {}\n",
    "            for k, v in encoded.items():\n",
    "                if isinstance(v, torch.Tensor):\n",
    "                    result[k] = v.squeeze(0)\n",
    "                else:\n",
    "                    result[k] = v\n",
    "\n",
    "            result[\"labels\"] = torch.tensor(sample[\"label\"], dtype=torch.long)\n",
    "            \n",
    "            # Clean up\n",
    "            del image, img_np, encoded\n",
    "            clear_memory()\n",
    "            \n",
    "            return result\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt + 1} failed for {sample['filename']}: {e}\")\n",
    "            clear_memory()\n",
    "            \n",
    "            if attempt == max_retries:\n",
    "                # Return dummy sample as last resort\n",
    "                print(f\"Creating dummy sample for {sample['filename']}\")\n",
    "                dummy_image = Image.new(\"RGB\", (224, 224), color=\"white\")\n",
    "                dummy = processor(\n",
    "                    images=dummy_image,\n",
    "                    text=[\"[ERROR]\"],\n",
    "                    boxes=[[0, 0, 50, 20]],\n",
    "                    truncation=True,\n",
    "                    padding=\"max_length\",\n",
    "                    max_length=MAX_LENGTH,\n",
    "                    return_tensors=\"pt\"\n",
    "                )\n",
    "                dummy = {k: v.squeeze(0) for k, v in dummy.items()}\n",
    "                dummy[\"labels\"] = torch.tensor(sample[\"label\"], dtype=torch.long)\n",
    "                return dummy\n",
    "\n",
    "# ---------------------- IMPROVED BATCH PROCESSING ----------------------\n",
    "def process_samples_in_batches(samples, batch_size=PROCESS_BATCH_SIZE):\n",
    "    \"\"\"Process samples in batches to manage memory with robust caching\"\"\"\n",
    "    processed_samples = []\n",
    "    \n",
    "    # Check for existing cache chunks\n",
    "    cache_pattern = os.path.join(CACHE_DIR, \"chunk_*.pkl\")\n",
    "    import glob\n",
    "    existing_chunks = sorted(glob.glob(cache_pattern))\n",
    "    \n",
    "    if existing_chunks:\n",
    "        print(f\"üì¶ Found {len(existing_chunks)} existing cache chunks\")\n",
    "        print(\"üîÑ Loading from cache chunks...\")\n",
    "        \n",
    "        for chunk_file in tqdm(existing_chunks, desc=\"Loading cache chunks\"):\n",
    "            chunk_data = safe_load_cache(chunk_file)\n",
    "            if chunk_data:\n",
    "                processed_samples.extend(chunk_data)\n",
    "            else:\n",
    "                print(f\"Skipping corrupted chunk: {os.path.basename(chunk_file)}\")\n",
    "        \n",
    "        print(f\"‚úÖ Loaded {len(processed_samples)} samples from cache\")\n",
    "        \n",
    "        # Verify we have all samples\n",
    "        if len(processed_samples) >= len(samples):\n",
    "            return processed_samples[:len(samples)]\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  Cache incomplete ({len(processed_samples)}/{len(samples)}), processing remaining...\")\n",
    "            remaining_samples = samples[len(processed_samples):]\n",
    "            start_chunk = len(existing_chunks)\n",
    "    else:\n",
    "        print(f\"üîÑ Processing {len(samples)} samples in batches of {batch_size}...\")\n",
    "        remaining_samples = samples\n",
    "        start_chunk = 0\n",
    "    \n",
    "    # Process remaining samples\n",
    "    if 'remaining_samples' in locals() and remaining_samples:\n",
    "        for i in tqdm(range(0, len(remaining_samples), batch_size), desc=\"Processing batches\"):\n",
    "            batch = remaining_samples[i:i + batch_size]\n",
    "            batch_processed = []\n",
    "            \n",
    "            for j, sample in enumerate(batch):\n",
    "                try:\n",
    "                    processed = process_single(sample)\n",
    "                    batch_processed.append(processed)\n",
    "                    \n",
    "                    # Progress update\n",
    "                    current_idx = len(processed_samples) + j + 1\n",
    "                    if current_idx % 100 == 0:\n",
    "                        mem_usage = get_memory_usage()\n",
    "                        print(f\"Processed {current_idx}/{len(samples)} samples. GPU memory: {mem_usage:.2f}GB\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to process sample {len(processed_samples) + j}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            # Save chunk immediately\n",
    "            chunk_num = start_chunk + (i // batch_size)\n",
    "            chunk_file = os.path.join(CACHE_DIR, f\"chunk_{chunk_num:04d}.pkl\")\n",
    "            \n",
    "            if safe_save_cache(batch_processed, chunk_file):\n",
    "                print(f\"üíæ Saved chunk {chunk_num} with {len(batch_processed)} samples\")\n",
    "            else:\n",
    "                print(f\"‚ùå Failed to save chunk {chunk_num}\")\n",
    "            \n",
    "            processed_samples.extend(batch_processed)\n",
    "            \n",
    "            # Clear memory after each batch\n",
    "            clear_memory()\n",
    "    \n",
    "    return processed_samples\n",
    "\n",
    "# ---------------------- CUSTOM DATASET ----------------------\n",
    "class MemoryEfficientDataset(TorchDataset):\n",
    "    def __init__(self, samples):\n",
    "        self.samples = samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        # Ensure proper tensor format\n",
    "        result = {}\n",
    "        for k, v in sample.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                result[k] = v.clone().detach()\n",
    "            else:\n",
    "                result[k] = torch.tensor(v)\n",
    "        return result\n",
    "\n",
    "# ---------------------- TRAIN & EVAL FUNCTIONS ----------------------\n",
    "def train_epoch(model, dataloader, optimizer, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc=f\"Training Epoch {epoch}\")\n",
    "    \n",
    "    for batch_idx, batch in enumerate(progress_bar):\n",
    "        try:\n",
    "            # Move to device\n",
    "            for k in batch:\n",
    "                batch[k] = batch[k].to(DEVICE, non_blocking=True)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            if torch.isnan(loss) or torch.isinf(loss):\n",
    "                print(f\"Skipping batch {batch_idx} due to invalid loss: {loss}\")\n",
    "                continue\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "            \n",
    "            # Clear cache periodically\n",
    "            if batch_idx % 20 == 0:\n",
    "                clear_memory()\n",
    "                \n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e):\n",
    "                print(f\"GPU OOM in batch {batch_idx}, clearing cache...\")\n",
    "                clear_memory()\n",
    "                continue\n",
    "            else:\n",
    "                print(f\"Runtime error in batch {batch_idx}: {e}\")\n",
    "                continue\n",
    "        except Exception as e:\n",
    "            print(f\"Error in batch {batch_idx}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    avg_loss = total_loss / max(num_batches, 1)\n",
    "    print(f\"Average training loss: {avg_loss:.4f}\")\n",
    "    return avg_loss\n",
    "\n",
    "def evaluate_model(model, dataloader, label_map, title=\"Validation\"):\n",
    "    model.eval()\n",
    "    preds, trues = [], []\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc=f\"Evaluating {title}\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(progress_bar):\n",
    "            try:\n",
    "                labels = batch['labels'].cpu().numpy()\n",
    "                \n",
    "                for k in batch:\n",
    "                    batch[k] = batch[k].to(DEVICE, non_blocking=True)\n",
    "                \n",
    "                outputs = model(**batch)\n",
    "                logits = outputs.logits\n",
    "                predictions = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "                \n",
    "                preds.extend(predictions)\n",
    "                trues.extend(labels)\n",
    "                \n",
    "                if batch_idx % 10 == 0:\n",
    "                    clear_memory()\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error in evaluation batch {batch_idx}: {e}\")\n",
    "                continue\n",
    "\n",
    "    if len(preds) == 0:\n",
    "        print(f\"No valid predictions for {title}\")\n",
    "        return\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(trues, preds)\n",
    "    f1 = f1_score(trues, preds, average='weighted', zero_division=0)\n",
    "    \n",
    "    print(f\"\\nüìä {title} Metrics:\")\n",
    "    print(f\"   Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"   F1-Score (weighted): {f1:.4f}\")\n",
    "    \n",
    "    print(f\"\\nüìã Classification Report ({title}):\")\n",
    "    print(classification_report(trues, preds, target_names=list(label_map.keys()), zero_division=0))\n",
    "\n",
    "    # Create confusion matrix plot\n",
    "    cm = confusion_matrix(trues, preds)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", \n",
    "                xticklabels=list(label_map.keys()), \n",
    "                yticklabels=list(label_map.keys()), \n",
    "                cmap=\"Blues\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(f\"Confusion Matrix ({title})\")\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save confusion matrix\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    plt.savefig(f\"confusion_matrix_{title.lower()}_{timestamp}.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()  # Important: close the figure to free memory\n",
    "    \n",
    "    return accuracy, f1\n",
    "\n",
    "def split_data(samples, train_ratio=0.8, val_ratio=0.1):\n",
    "    \"\"\"Split data into train, validation, and test sets\"\"\"\n",
    "    np.random.shuffle(samples)\n",
    "    n = len(samples)\n",
    "    \n",
    "    train_end = int(n * train_ratio)\n",
    "    val_end = int(n * (train_ratio + val_ratio))\n",
    "    \n",
    "    train_samples = samples[:train_end]\n",
    "    val_samples = samples[train_end:val_end]\n",
    "    test_samples = samples[val_end:]\n",
    "    \n",
    "    print(f\"üìä Data Split:\")\n",
    "    print(f\"   Training: {len(train_samples)} samples\")\n",
    "    print(f\"   Validation: {len(val_samples)} samples\")\n",
    "    print(f\"   Test: {len(test_samples)} samples\")\n",
    "    \n",
    "    return train_samples, val_samples, test_samples\n",
    "\n",
    "def save_model_and_results(model, label_map, results, timestamp):\n",
    "    \"\"\"Save model and training results\"\"\"\n",
    "    try:\n",
    "        # Save model\n",
    "        model_path = os.path.join(SAVE_DIR, f\"layoutlm_model_{timestamp}\")\n",
    "        model.save_pretrained(model_path)\n",
    "        \n",
    "        # Save processor\n",
    "        processor.save_pretrained(model_path)\n",
    "        \n",
    "        # Save label map and results\n",
    "        metadata = {\n",
    "            'label_map': label_map,\n",
    "            'results': results,\n",
    "            'model_name': MODEL_NAME,\n",
    "            'config': {\n",
    "                'max_length': MAX_LENGTH,\n",
    "                'batch_size': BATCH_SIZE,\n",
    "                'epochs': EPOCHS,\n",
    "                'learning_rate': LEARNING_RATE\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        metadata_path = os.path.join(model_path, 'metadata.json')\n",
    "        with open(metadata_path, 'w') as f:\n",
    "            json.dump(metadata, f, indent=2, default=str)\n",
    "        \n",
    "        print(f\"üíæ Model and metadata saved to: {model_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to save model: {e}\")\n",
    "\n",
    "# ---------------------- MAIN FUNCTION ----------------------\n",
    "def main(data_dir=None, epochs=None, batch_size=None, learning_rate=None, no_cache=False):\n",
    "    \"\"\"\n",
    "    Main training function that can be called directly or via command line\n",
    "    \"\"\"\n",
    "    # Use provided parameters or fall back to globals\n",
    "    data_dir = data_dir or DATA_DIR\n",
    "    epochs = epochs or EPOCHS\n",
    "    batch_size = batch_size or BATCH_SIZE\n",
    "    learning_rate = learning_rate or LEARNING_RATE\n",
    "    \n",
    "    print(f\"üöÄ Starting LayoutLM Document Classification Training\")\n",
    "    print(f\"   Data directory: {data_dir}\")\n",
    "    print(f\"   Epochs: {epochs}\")\n",
    "    print(f\"   Batch size: {batch_size}\")\n",
    "    print(f\"   Learning rate: {learning_rate}\")\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Load data\n",
    "    print(\"\\nüìÇ Loading dataset...\")\n",
    "    samples, label_map = load_data(data_dir)\n",
    "    \n",
    "    # Split data\n",
    "    train_samples, val_samples, test_samples = split_data(samples)\n",
    "    \n",
    "    # Process samples with improved caching\n",
    "    if no_cache:\n",
    "        print(\"üóëÔ∏è  Clearing cache as requested...\")\n",
    "        import glob\n",
    "        cache_files = glob.glob(os.path.join(CACHE_DIR, \"chunk_*.pkl\"))\n",
    "        for cache_file in cache_files:\n",
    "            try:\n",
    "                os.unlink(cache_file)\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    print(\"üîÑ Processing samples with robust caching...\")\n",
    "    all_processed = process_samples_in_batches(samples)\n",
    "    \n",
    "    # Verify processing completed\n",
    "    if len(all_processed) != len(samples):\n",
    "        print(f\"‚ö†Ô∏è  Warning: Only {len(all_processed)}/{len(samples)} samples processed successfully\")\n",
    "    \n",
    "    # Split processed samples according to original split\n",
    "    train_processed = all_processed[:len(train_samples)]\n",
    "    val_processed = all_processed[len(train_samples):len(train_samples)+len(val_samples)]\n",
    "    test_processed = all_processed[len(train_samples)+len(val_samples):]\n",
    "    \n",
    "    # Create datasets and dataloaders\n",
    "    print(\"\\nüîß Creating datasets and dataloaders...\")\n",
    "    train_dataset = MemoryEfficientDataset(train_processed)\n",
    "    val_dataset = MemoryEfficientDataset(val_processed)\n",
    "    test_dataset = MemoryEfficientDataset(test_processed)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, \n",
    "                             num_workers=0, pin_memory=True if USE_GPU else False)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, \n",
    "                           num_workers=0, pin_memory=True if USE_GPU else False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, \n",
    "                            num_workers=0, pin_memory=True if USE_GPU else False)\n",
    "    \n",
    "    # Initialize model\n",
    "    print(\"\\nü§ñ Initializing model...\")\n",
    "    num_labels = len(label_map)\n",
    "    model = MODEL_CLASS.from_pretrained(MODEL_NAME, num_labels=num_labels)\n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    # Initialize optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Training loop\n",
    "    print(f\"\\nüéØ Starting training...\")\n",
    "    training_losses = []\n",
    "    validation_accuracies = []\n",
    "    validation_f1_scores = []\n",
    "    \n",
    "    best_f1 = 0.0\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Epoch {epoch}/{epochs}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Train\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, epoch)\n",
    "        training_losses.append(train_loss)\n",
    "        \n",
    "        # Validate\n",
    "        if len(val_processed) > 0:\n",
    "            val_acc, val_f1 = evaluate_model(model, val_loader, label_map, f\"Validation Epoch {epoch}\")\n",
    "            validation_accuracies.append(val_acc)\n",
    "            validation_f1_scores.append(val_f1)\n",
    "            \n",
    "            # Save best model\n",
    "            if val_f1 > best_f1:\n",
    "                best_f1 = val_f1\n",
    "                best_model_state = model.state_dict().copy()\n",
    "                print(f\"üèÜ New best model! F1: {best_f1:.4f}\")\n",
    "        \n",
    "        # Memory cleanup\n",
    "        clear_memory()\n",
    "    \n",
    "    # Load best model for final evaluation\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(f\"\\nüèÜ Loaded best model (F1: {best_f1:.4f}) for final evaluation\")\n",
    "    \n",
    "    # Final evaluation on test set\n",
    "    if len(test_processed) > 0:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(\"FINAL TEST EVALUATION\")\n",
    "        print(f\"{'='*50}\")\n",
    "        test_acc, test_f1 = evaluate_model(model, test_loader, label_map, \"Test Set\")\n",
    "    else:\n",
    "        test_acc, test_f1 = 0.0, 0.0\n",
    "    \n",
    "    # Create training plots\n",
    "    if len(training_losses) > 1:\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        \n",
    "        # Loss plot\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(range(1, len(training_losses) + 1), training_losses, 'b-', label='Training Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        \n",
    "        # Metrics plot\n",
    "        if validation_accuracies:\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.plot(range(1, len(validation_accuracies) + 1), validation_accuracies, 'g-', label='Validation Accuracy')\n",
    "            plt.plot(range(1, len(validation_f1_scores) + 1), validation_f1_scores, 'r-', label='Validation F1')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Score')\n",
    "            plt.title('Validation Metrics')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"training_progress_{timestamp}.png\", dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    \n",
    "    # Save results\n",
    "    results = {\n",
    "        'training_losses': training_losses,\n",
    "        'validation_accuracies': validation_accuracies,\n",
    "        'validation_f1_scores': validation_f1_scores,\n",
    "        'test_accuracy': test_acc,\n",
    "        'test_f1': test_f1,\n",
    "        'best_validation_f1': best_f1,\n",
    "        'label_map': label_map,\n",
    "        'timestamp': timestamp\n",
    "    }\n",
    "    \n",
    "    save_model_and_results(model, label_map, results, timestamp)\n",
    "    \n",
    "    print(f\"\\nüéâ Training completed!\")\n",
    "    print(f\"   Best validation F1: {best_f1:.4f}\")\n",
    "    print(f\"   Test accuracy: {test_acc:.4f}\")\n",
    "    print(f\"   Test F1: {test_f1:.4f}\")\n",
    "    print(f\"   Model and results saved with timestamp: {timestamp}\")\n",
    "\n",
    "def main_with_args():\n",
    "    \"\"\"Command line version with argparse\"\"\"\n",
    "    DEFAULT_DATA_DIR = DATA_DIR\n",
    "    DEFAULT_EPOCHS = EPOCHS\n",
    "    DEFAULT_BATCH_SIZE = BATCH_SIZE\n",
    "    DEFAULT_LEARNING_RATE = LEARNING_RATE\n",
    "    \n",
    "    parser = argparse.ArgumentParser(description='Train LayoutLM for document classification')\n",
    "    parser.add_argument('--data_dir', type=str, default=DEFAULT_DATA_DIR, help='Path to dataset directory')\n",
    "    parser.add_argument('--epochs', type=int, default=DEFAULT_EPOCHS, help='Number of training epochs')\n",
    "    parser.add_argument('--batch_size', type=int, default=DEFAULT_BATCH_SIZE, help='Batch size')\n",
    "    parser.add_argument('--learning_rate', type=float, default=DEFAULT_LEARNING_RATE, help='Learning rate')\n",
    "    parser.add_argument('--no_cache', action='store_true', help='Skip using cached processed samples')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    main(args.data_dir, args.epochs, args.batch_size, args.learning_rate, args.no_cache)\n",
    "\n",
    "# For command line usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Check if we're in a Jupyter notebook\n",
    "    try:\n",
    "        # This will be True if running in Jupyter\n",
    "        get_ipython()\n",
    "        print(\"üîç Detected Jupyter environment - running with default parameters\")\n",
    "        print(\"üí° To use custom parameters, call: main(epochs=5, batch_size=8, etc.)\")\n",
    "        main()  # Run with defaults in Jupyter\n",
    "    except NameError:\n",
    "        # We're in a regular Python script\n",
    "        main_with_args()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (gpu_ml)",
   "language": "python",
   "name": "gpu_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
