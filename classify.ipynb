{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10a728ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers version: 4.52.4\n",
      "✅ Using LayoutLMv3\n",
      "🔧 Configuration:\n",
      "   Model: microsoft/layoutlmv3-base\n",
      "   Device: cuda\n",
      "   Batch size: 2\n",
      "   Max length: 128\n",
      "   Learning rate: 2e-05\n",
      "   Epochs: 3\n",
      "✅ Processor loaded successfully\n",
      "✅ EasyOCR initialized successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset as TorchDataset, DataLoader\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import easyocr\n",
    "import gc\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "\n",
    "# Check transformers version and import appropriate LayoutLM model\n",
    "try:\n",
    "    import transformers\n",
    "    print(f\"Transformers version: {transformers.__version__}\")\n",
    "    \n",
    "    # Try to import LayoutLMv3 first (requires transformers >= 4.21.0)\n",
    "    try:\n",
    "        from transformers import LayoutLMv3Processor, LayoutLMv3ForSequenceClassification\n",
    "        MODEL_NAME = \"microsoft/layoutlmv3-base\"\n",
    "        PROCESSOR_CLASS = LayoutLMv3Processor\n",
    "        MODEL_CLASS = LayoutLMv3ForSequenceClassification\n",
    "        print(\"✅ Using LayoutLMv3\")\n",
    "    except ImportError:\n",
    "        print(\"❌ LayoutLMv3 not available, trying LayoutLMv2...\")\n",
    "        try:\n",
    "            from transformers import LayoutLMv2Processor, LayoutLMv2ForSequenceClassification\n",
    "            MODEL_NAME = \"microsoft/layoutlmv2-base-uncased\"\n",
    "            PROCESSOR_CLASS = LayoutLMv2Processor\n",
    "            MODEL_CLASS = LayoutLMv2ForSequenceClassification\n",
    "            print(\"✅ Using LayoutLMv2\")\n",
    "        except ImportError:\n",
    "            print(\"❌ Neither LayoutLMv3 nor LayoutLMv2 available\")\n",
    "            print(\"📝 Please upgrade transformers: pip install transformers>=4.21.0\")\n",
    "            raise ImportError(\"LayoutLM models not available. Please upgrade transformers library.\")\n",
    "            \n",
    "except ImportError as e:\n",
    "    print(f\"❌ Transformers library not found: {e}\")\n",
    "    print(\"📝 Please install transformers: pip install transformers>=4.21.0\")\n",
    "    raise\n",
    "\n",
    "# ---------------------- CONFIG ----------------------\n",
    "DATA_DIR = \"/home/hasan/datasets/classify/test\"\n",
    "MAX_LENGTH = 128\n",
    "BATCH_SIZE = 2  # Reduced batch size\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 2e-5\n",
    "USE_GPU = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda\" if USE_GPU else \"cpu\")\n",
    "CACHE_DIR = \"./cache\"  # Directory to cache processed samples\n",
    "PROCESS_BATCH_SIZE = 50  # Process samples in smaller batches\n",
    "SAVE_DIR = \"./models\"  # Directory to save trained models\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"🔧 Configuration:\")\n",
    "print(f\"   Model: {MODEL_NAME}\")\n",
    "print(f\"   Device: {DEVICE}\")\n",
    "print(f\"   Batch size: {BATCH_SIZE}\")\n",
    "print(f\"   Max length: {MAX_LENGTH}\")\n",
    "print(f\"   Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"   Epochs: {EPOCHS}\")\n",
    "\n",
    "# ---------------------- INIT ----------------------\n",
    "try:\n",
    "    processor = PROCESSOR_CLASS.from_pretrained(MODEL_NAME, apply_ocr=False)\n",
    "    print(\"✅ Processor loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to load processor: {e}\")\n",
    "    raise\n",
    "\n",
    "# Initialize EasyOCR with limited GPU memory\n",
    "try:\n",
    "    reader = easyocr.Reader(['en'], gpu=USE_GPU, model_storage_directory='./easyocr_models')\n",
    "    print(\"✅ EasyOCR initialized successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to initialize EasyOCR: {e}\")\n",
    "    print(\"📝 Please install EasyOCR: pip install easyocr\")\n",
    "    raise\n",
    "\n",
    "# ---------------------- MEMORY MANAGEMENT ----------------------\n",
    "def clear_memory():\n",
    "    \"\"\"Clear GPU and system memory\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "def get_memory_usage():\n",
    "    \"\"\"Get current GPU memory usage\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.cuda.memory_allocated() / 1024**3  # GB\n",
    "    return 0\n",
    "\n",
    "# ---------------------- LOAD DATA ----------------------\n",
    "def load_data(data_dir):\n",
    "    if not os.path.exists(data_dir):\n",
    "        raise FileNotFoundError(f\"Data directory not found: {data_dir}\")\n",
    "    \n",
    "    samples = []\n",
    "    label_map = {}\n",
    "    label_id = 0\n",
    "\n",
    "    for label_name in sorted(os.listdir(data_dir)):\n",
    "        label_path = os.path.join(data_dir, label_name)\n",
    "        if not os.path.isdir(label_path):\n",
    "            continue\n",
    "\n",
    "        if label_name not in label_map:\n",
    "            label_map[label_name] = label_id\n",
    "            label_id += 1\n",
    "\n",
    "        for file in os.listdir(label_path):\n",
    "            if file.lower().endswith((\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tiff\")):\n",
    "                samples.append({\n",
    "                    \"image_path\": os.path.join(label_path, file),\n",
    "                    \"label\": label_map[label_name],\n",
    "                    \"filename\": file\n",
    "                })\n",
    "\n",
    "    print(f\"📊 Dataset Summary:\")\n",
    "    print(f\"   Unique labels: {sorted(label_map.items(), key=lambda x: x[1])}\")\n",
    "    print(f\"   Total samples: {len(samples)}\")\n",
    "    \n",
    "    if len(samples) == 0:\n",
    "        raise ValueError(\"No image files found in the dataset directory\")\n",
    "    \n",
    "    return samples, label_map\n",
    "\n",
    "# ---------------------- PROCESS SAMPLE ----------------------\n",
    "def process_single(sample, max_retries=2):\n",
    "    \"\"\"Process a single sample with error handling and retries\"\"\"\n",
    "    for attempt in range(max_retries + 1):\n",
    "        try:\n",
    "            image_path = sample[\"image_path\"]\n",
    "            \n",
    "            # Load and resize image if too large\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            \n",
    "            # Resize very large images to prevent memory issues\n",
    "            max_size = 2048\n",
    "            if max(image.size) > max_size:\n",
    "                ratio = max_size / max(image.size)\n",
    "                new_size = tuple(int(dim * ratio) for dim in image.size)\n",
    "                image = image.resize(new_size, Image.Resampling.LANCZOS)\n",
    "            \n",
    "            img_np = np.array(image)\n",
    "            \n",
    "            # OCR with error handling\n",
    "            try:\n",
    "                results = reader.readtext(img_np)\n",
    "            except Exception as ocr_error:\n",
    "                print(f\"OCR failed for {sample['filename']}: {ocr_error}\")\n",
    "                results = []\n",
    "            \n",
    "            words, boxes = [], []\n",
    "            \n",
    "            for box, text, conf in results:\n",
    "                if conf > 0.5 and text.strip():\n",
    "                    words.append(text.strip())\n",
    "                    x0 = min(pt[0] for pt in box)\n",
    "                    y0 = min(pt[1] for pt in box)\n",
    "                    x1 = max(pt[0] for pt in box)\n",
    "                    y1 = max(pt[1] for pt in box)\n",
    "                    boxes.append([int(x0), int(y0), int(x1), int(y1)])\n",
    "\n",
    "            # Fallback for images with no text\n",
    "            if not words:\n",
    "                words = [\"[EMPTY]\"]\n",
    "                boxes = [[0, 0, 50, 20]]\n",
    "\n",
    "            # Limit number of words to prevent memory issues\n",
    "            if len(words) > 100:\n",
    "                words = words[:100]\n",
    "                boxes = boxes[:100]\n",
    "\n",
    "            # Process with the appropriate processor\n",
    "            encoded = processor(\n",
    "                images=image,\n",
    "                text=words,\n",
    "                boxes=boxes,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                max_length=MAX_LENGTH,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "\n",
    "            # Process encoded data\n",
    "            result = {}\n",
    "            for k, v in encoded.items():\n",
    "                if isinstance(v, torch.Tensor):\n",
    "                    result[k] = v.squeeze(0)\n",
    "                else:\n",
    "                    result[k] = v\n",
    "\n",
    "            result[\"labels\"] = torch.tensor(sample[\"label\"], dtype=torch.long)\n",
    "            \n",
    "            # Clean up\n",
    "            del image, img_np, encoded\n",
    "            clear_memory()\n",
    "            \n",
    "            return result\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt + 1} failed for {sample['filename']}: {e}\")\n",
    "            clear_memory()\n",
    "            \n",
    "            if attempt == max_retries:\n",
    "                # Return dummy sample as last resort\n",
    "                print(f\"Creating dummy sample for {sample['filename']}\")\n",
    "                dummy_image = Image.new(\"RGB\", (224, 224), color=\"white\")\n",
    "                dummy = processor(\n",
    "                    images=dummy_image,\n",
    "                    text=[\"[ERROR]\"],\n",
    "                    boxes=[[0, 0, 50, 20]],\n",
    "                    truncation=True,\n",
    "                    padding=\"max_length\",\n",
    "                    max_length=MAX_LENGTH,\n",
    "                    return_tensors=\"pt\"\n",
    "                )\n",
    "                dummy = {k: v.squeeze(0) for k, v in dummy.items()}\n",
    "                dummy[\"labels\"] = torch.tensor(sample[\"label\"], dtype=torch.long)\n",
    "                return dummy\n",
    "\n",
    "# ---------------------- BATCH PROCESSING ----------------------\n",
    "def process_samples_in_batches(samples, batch_size=PROCESS_BATCH_SIZE):\n",
    "    \"\"\"Process samples in batches to manage memory\"\"\"\n",
    "    processed_samples = []\n",
    "    \n",
    "    print(f\"🔄 Processing {len(samples)} samples in batches of {batch_size}...\")\n",
    "    \n",
    "    for i in tqdm(range(0, len(samples), batch_size), desc=\"Processing batches\"):\n",
    "        batch = samples[i:i + batch_size]\n",
    "        batch_processed = []\n",
    "        \n",
    "        for j, sample in enumerate(batch):\n",
    "            try:\n",
    "                processed = process_single(sample)\n",
    "                batch_processed.append(processed)\n",
    "                \n",
    "                # Progress update\n",
    "                current_idx = i + j + 1\n",
    "                if current_idx % 100 == 0:\n",
    "                    mem_usage = get_memory_usage()\n",
    "                    print(f\"Processed {current_idx}/{len(samples)} samples. GPU memory: {mem_usage:.2f}GB\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Failed to process sample {i + j}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        processed_samples.extend(batch_processed)\n",
    "        \n",
    "        # Clear memory after each batch\n",
    "        clear_memory()\n",
    "        \n",
    "        # Save progress periodically\n",
    "        if (i // batch_size + 1) % 10 == 0:\n",
    "            cache_file = os.path.join(CACHE_DIR, f\"processed_batch_{i//batch_size + 1}.pkl\")\n",
    "            with open(cache_file, 'wb') as f:\n",
    "                pickle.dump(batch_processed, f)\n",
    "            print(f\"💾 Saved progress to {cache_file}\")\n",
    "    \n",
    "    return processed_samples\n",
    "\n",
    "# ---------------------- CUSTOM DATASET ----------------------\n",
    "class MemoryEfficientDataset(TorchDataset):\n",
    "    def __init__(self, samples):\n",
    "        self.samples = samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        # Ensure proper tensor format\n",
    "        result = {}\n",
    "        for k, v in sample.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                result[k] = v.clone().detach()\n",
    "            else:\n",
    "                result[k] = torch.tensor(v)\n",
    "        return result\n",
    "\n",
    "# ---------------------- TRAIN & EVAL FUNCTIONS ----------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5349a611",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc=f\"Training Epoch {epoch}\")\n",
    "    \n",
    "    for batch_idx, batch in enumerate(progress_bar):\n",
    "        try:\n",
    "            # Move to device\n",
    "            for k in batch:\n",
    "                batch[k] = batch[k].to(DEVICE, non_blocking=True)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            if torch.isnan(loss) or torch.isinf(loss):\n",
    "                print(f\"Skipping batch {batch_idx} due to invalid loss: {loss}\")\n",
    "                continue\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "            \n",
    "            # Clear cache periodically\n",
    "            if batch_idx % 20 == 0:\n",
    "                clear_memory()\n",
    "                \n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e):\n",
    "                print(f\"GPU OOM in batch {batch_idx}, clearing cache...\")\n",
    "                clear_memory()\n",
    "                continue\n",
    "            else:\n",
    "                print(f\"Runtime error in batch {batch_idx}: {e}\")\n",
    "                continue\n",
    "        except Exception as e:\n",
    "            print(f\"Error in batch {batch_idx}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    avg_loss = total_loss / max(num_batches, 1)\n",
    "    print(f\"Average training loss: {avg_loss:.4f}\")\n",
    "    return avg_loss\n",
    "\n",
    "def evaluate_model(model, dataloader, label_map, title=\"Validation\"):\n",
    "    model.eval()\n",
    "    preds, trues = [], []\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc=f\"Evaluating {title}\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(progress_bar):\n",
    "            try:\n",
    "                labels = batch['labels'].cpu().numpy()\n",
    "                \n",
    "                for k in batch:\n",
    "                    batch[k] = batch[k].to(DEVICE, non_blocking=True)\n",
    "                \n",
    "                outputs = model(**batch)\n",
    "                logits = outputs.logits\n",
    "                predictions = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "                \n",
    "                preds.extend(predictions)\n",
    "                trues.extend(labels)\n",
    "                \n",
    "                if batch_idx % 10 == 0:\n",
    "                    clear_memory()\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error in evaluation batch {batch_idx}: {e}\")\n",
    "                continue\n",
    "\n",
    "    if len(preds) == 0:\n",
    "        print(f\"No valid predictions for {title}\")\n",
    "        return\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(trues, preds)\n",
    "    f1 = f1_score(trues, preds, average='weighted', zero_division=0)\n",
    "    \n",
    "    print(f\"\\n📊 {title} Metrics:\")\n",
    "    print(f\"   Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"   F1-Score (weighted): {f1:.4f}\")\n",
    "    \n",
    "    print(f\"\\n📋 Classification Report ({title}):\")\n",
    "    print(classification_report(trues, preds, target_names=list(label_map.keys()), zero_division=0))\n",
    "\n",
    "    # Create confusion matrix plot\n",
    "    cm = confusion_matrix(trues, preds)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", \n",
    "                xticklabels=list(label_map.keys()), \n",
    "                yticklabels=list(label_map.keys()), \n",
    "                cmap=\"Blues\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(f\"Confusion Matrix ({title})\")\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save confusion matrix\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    plt.savefig(f\"confusion_matrix_{title.lower()}_{timestamp}.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()  # Important: close the figure to free memory\n",
    "    \n",
    "    return accuracy, f1\n",
    "\n",
    "def split_data(samples, train_ratio=0.8, val_ratio=0.1):\n",
    "    \"\"\"Split data into train, validation, and test sets\"\"\"\n",
    "    np.random.shuffle(samples)\n",
    "    n = len(samples)\n",
    "    \n",
    "    train_end = int(n * train_ratio)\n",
    "    val_end = int(n * (train_ratio + val_ratio))\n",
    "    \n",
    "    train_samples = samples[:train_end]\n",
    "    val_samples = samples[train_end:val_end]\n",
    "    test_samples = samples[val_end:]\n",
    "    \n",
    "    print(f\"📊 Data Split:\")\n",
    "    print(f\"   Training: {len(train_samples)} samples\")\n",
    "    print(f\"   Validation: {len(val_samples)} samples\")\n",
    "    print(f\"   Test: {len(test_samples)} samples\")\n",
    "    \n",
    "    return train_samples, val_samples, test_samples\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6956e088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Detected Jupyter environment - running with default parameters\n",
      "💡 To use custom parameters, call: main(epochs=5, batch_size=8, etc.)\n",
      "🚀 Starting LayoutLM Document Classification Training\n",
      "   Data directory: /home/hasan/datasets/classify/test\n",
      "   Epochs: 3\n",
      "   Batch size: 2\n",
      "   Learning rate: 2e-05\n",
      "\n",
      "📂 Loading dataset...\n",
      "📊 Dataset Summary:\n",
      "   Unique labels: [('ID', 0), ('advertisement', 1), ('budget', 2), ('email', 3), ('file_folder', 4), ('form', 5), ('handwritten', 6), ('invoice', 7), ('letter', 8), ('memo', 9), ('news_article', 10), ('presentation', 11), ('questionnaire', 12), ('receipt', 13), ('resume', 14), ('scientific_publication', 15), ('scientific_report', 16), ('specification', 17)]\n",
      "   Total samples: 3244\n",
      "📊 Data Split:\n",
      "   Training: 2595 samples\n",
      "   Validation: 324 samples\n",
      "   Test: 325 samples\n",
      "🔄 Processing samples...\n",
      "🔄 Processing 3244 samples in batches of 50...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   3%|█▋                                                    | 2/65 [01:40<51:52, 49.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100/3244 samples. GPU memory: 0.10GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   6%|███▎                                                  | 4/65 [03:31<54:22, 53.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 200/3244 samples. GPU memory: 0.10GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   9%|████▉                                                 | 6/65 [05:17<52:27, 53.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 300/3244 samples. GPU memory: 0.10GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  12%|██████▋                                               | 8/65 [07:02<49:58, 52.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 400/3244 samples. GPU memory: 0.10GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing batches:  14%|███████▍                                              | 9/65 [07:50<47:46, 51.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 500/3244 samples. GPU memory: 0.10GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing batches:  15%|████████▏                                            | 10/65 [08:31<43:54, 47.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saved progress to ./cache/processed_batch_10.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  18%|█████████▊                                           | 12/65 [10:12<43:48, 49.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 600/3244 samples. GPU memory: 0.10GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  22%|███████████▍                                         | 14/65 [11:58<43:12, 50.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 700/3244 samples. GPU memory: 0.10GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  25%|█████████████                                        | 16/65 [13:34<40:33, 49.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 800/3244 samples. GPU memory: 0.10GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  28%|██████████████▋                                      | 18/65 [15:10<38:06, 48.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 900/3244 samples. GPU memory: 0.10GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing batches:  29%|███████████████▍                                     | 19/65 [16:01<37:45, 49.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000/3244 samples. GPU memory: 0.10GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing batches:  31%|████████████████▎                                    | 20/65 [16:54<37:52, 50.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saved progress to ./cache/processed_batch_20.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  34%|█████████████████▉                                   | 22/65 [18:48<38:23, 53.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1100/3244 samples. GPU memory: 0.10GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  37%|███████████████████▌                                 | 24/65 [20:25<34:46, 50.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1200/3244 samples. GPU memory: 0.10GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  40%|█████████████████████▏                               | 26/65 [22:12<34:15, 52.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1300/3244 samples. GPU memory: 0.10GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  43%|██████████████████████▊                              | 28/65 [23:40<29:55, 48.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1400/3244 samples. GPU memory: 0.10GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  46%|████████████████████████▍                            | 30/65 [25:29<30:00, 51.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1500/3244 samples. GPU memory: 0.10GB\n",
      "💾 Saved progress to ./cache/processed_batch_30.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  49%|██████████████████████████                           | 32/65 [27:13<28:14, 51.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1600/3244 samples. GPU memory: 0.10GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  52%|███████████████████████████▋                         | 34/65 [28:54<26:14, 50.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1700/3244 samples. GPU memory: 0.10GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  55%|█████████████████████████████▎                       | 36/65 [30:44<25:33, 52.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1800/3244 samples. GPU memory: 0.10GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing batches:  57%|██████████████████████████████▏                      | 37/65 [31:35<24:28, 52.45s/it]"
     ]
    }
   ],
   "source": [
    "def main(data_dir=None, epochs=None, batch_size=None, learning_rate=None, no_cache=False):\n",
    "    \"\"\"\n",
    "    Main training function that can be called directly or via command line\n",
    "    \"\"\"\n",
    "    # Use provided parameters or fall back to globals\n",
    "    data_dir = data_dir or DATA_DIR\n",
    "    epochs = epochs or EPOCHS\n",
    "    batch_size = batch_size or BATCH_SIZE\n",
    "    learning_rate = learning_rate or LEARNING_RATE\n",
    "    \n",
    "    print(f\"🚀 Starting LayoutLM Document Classification Training\")\n",
    "    print(f\"   Data directory: {data_dir}\")\n",
    "    print(f\"   Epochs: {epochs}\")\n",
    "    print(f\"   Batch size: {batch_size}\")\n",
    "    print(f\"   Learning rate: {learning_rate}\")\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Load data\n",
    "    print(\"\\n📂 Loading dataset...\")\n",
    "    samples, label_map = load_data(data_dir)\n",
    "    \n",
    "    # Split data\n",
    "    train_samples, val_samples, test_samples = split_data(samples)\n",
    "    \n",
    "    # Process samples (check for cached versions first)\n",
    "    cache_file = os.path.join(CACHE_DIR, \"all_processed_samples.pkl\")\n",
    "    \n",
    "    if not no_cache and os.path.exists(cache_file):\n",
    "        print(f\"📦 Loading cached processed samples from {cache_file}\")\n",
    "        with open(cache_file, 'rb') as f:\n",
    "            cached_data = pickle.load(f)\n",
    "            all_processed = cached_data['samples']\n",
    "            cached_label_map = cached_data['label_map']\n",
    "            \n",
    "        # Verify label map consistency\n",
    "        if cached_label_map != label_map:\n",
    "            print(\"⚠️  Label map mismatch, reprocessing samples...\")\n",
    "            all_processed = process_samples_in_batches(samples)\n",
    "            # Save processed samples\n",
    "            with open(cache_file, 'wb') as f:\n",
    "                pickle.dump({'samples': all_processed, 'label_map': label_map}, f)\n",
    "    else:\n",
    "        print(\"🔄 Processing samples...\")\n",
    "        all_processed = process_samples_in_batches(samples)\n",
    "        # Save processed samples\n",
    "        with open(cache_file, 'wb') as f:\n",
    "            pickle.dump({'samples': all_processed, 'label_map': label_map}, f)\n",
    "        print(f\"💾 Cached processed samples to {cache_file}\")\n",
    "    \n",
    "    # Split processed samples according to original split\n",
    "    train_processed = all_processed[:len(train_samples)]\n",
    "    val_processed = all_processed[len(train_samples):len(train_samples)+len(val_samples)]\n",
    "    test_processed = all_processed[len(train_samples)+len(val_samples):]\n",
    "    \n",
    "    # Create datasets and dataloaders\n",
    "    print(\"\\n🔧 Creating datasets and dataloaders...\")\n",
    "    train_dataset = MemoryEfficientDataset(train_processed)\n",
    "    val_dataset = MemoryEfficientDataset(val_processed)\n",
    "    test_dataset = MemoryEfficientDataset(test_processed)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, \n",
    "                             num_workers=0, pin_memory=True if USE_GPU else False)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, \n",
    "                           num_workers=0, pin_memory=True if USE_GPU else False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, \n",
    "                            num_workers=0, pin_memory=True if USE_GPU else False)\n",
    "    \n",
    "    # Initialize model\n",
    "    print(\"\\n🤖 Initializing model...\")\n",
    "    num_labels = len(label_map)\n",
    "    model = MODEL_CLASS.from_pretrained(MODEL_NAME, num_labels=num_labels)\n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    # Initialize optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Training loop\n",
    "    print(f\"\\n🎯 Starting training...\")\n",
    "    training_losses = []\n",
    "    validation_accuracies = []\n",
    "    validation_f1_scores = []\n",
    "    \n",
    "    best_f1 = 0.0\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Epoch {epoch}/{epochs}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Train\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, epoch)\n",
    "        training_losses.append(train_loss)\n",
    "        \n",
    "        # Validate\n",
    "        if len(val_processed) > 0:\n",
    "            val_acc, val_f1 = evaluate_model(model, val_loader, label_map, f\"Validation Epoch {epoch}\")\n",
    "            validation_accuracies.append(val_acc)\n",
    "            validation_f1_scores.append(val_f1)\n",
    "            \n",
    "            # Save best model\n",
    "            if val_f1 > best_f1:\n",
    "                best_f1 = val_f1\n",
    "                best_model_state = model.state_dict().copy()\n",
    "                print(f\"🏆 New best model! F1: {best_f1:.4f}\")\n",
    "        \n",
    "        # Memory cleanup\n",
    "        clear_memory()\n",
    "    \n",
    "    # Load best model for final evaluation\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(f\"\\n🏆 Loaded best model (F1: {best_f1:.4f}) for final evaluation\")\n",
    "    \n",
    "    # Final evaluation on test set\n",
    "    if len(test_processed) > 0:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(\"FINAL TEST EVALUATION\")\n",
    "        print(f\"{'='*50}\")\n",
    "        test_acc, test_f1 = evaluate_model(model, test_loader, label_map, \"Test Set\")\n",
    "    else:\n",
    "        test_acc, test_f1 = 0.0, 0.0\n",
    "    \n",
    "    # Create training plots\n",
    "    if len(training_losses) > 1:\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        \n",
    "        # Loss plot\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(range(1, len(training_losses) + 1), training_losses, 'b-', label='Training Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        \n",
    "        # Metrics plot\n",
    "        if validation_accuracies:\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.plot(range(1, len(validation_accuracies) + 1), validation_accuracies, 'g-', label='Validation Accuracy')\n",
    "            plt.plot(range(1, len(validation_f1_scores) + 1), validation_f1_scores, 'r-', label='Validation F1')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Score')\n",
    "            plt.title('Validation Metrics')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"training_progress_{timestamp}.png\", dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    \n",
    "    # Save results\n",
    "    results = {\n",
    "        'training_losses': training_losses,\n",
    "        'validation_accuracies': validation_accuracies,\n",
    "        'validation_f1_scores': validation_f1_scores,\n",
    "        'test_accuracy': test_acc,\n",
    "        'test_f1': test_f1,\n",
    "        'best_validation_f1': best_f1,\n",
    "        'label_map': label_map,\n",
    "        'timestamp': timestamp\n",
    "    }\n",
    "    \n",
    "    save_model_and_results(model, label_map, results, timestamp)\n",
    "    \n",
    "    print(f\"\\n🎉 Training completed!\")\n",
    "    print(f\"   Best validation F1: {best_f1:.4f}\")\n",
    "    print(f\"   Test accuracy: {test_acc:.4f}\")\n",
    "    print(f\"   Test F1: {test_f1:.4f}\")\n",
    "    print(f\"   Model and results saved with timestamp: {timestamp}\")\n",
    "\n",
    "def main_with_args():\n",
    "    \"\"\"Command line version with argparse\"\"\"\n",
    "    DEFAULT_DATA_DIR = DATA_DIR\n",
    "    DEFAULT_EPOCHS = EPOCHS\n",
    "    DEFAULT_BATCH_SIZE = BATCH_SIZE\n",
    "    DEFAULT_LEARNING_RATE = LEARNING_RATE\n",
    "    \n",
    "    parser = argparse.ArgumentParser(description='Train LayoutLM for document classification')\n",
    "    parser.add_argument('--data_dir', type=str, default=DEFAULT_DATA_DIR, help='Path to dataset directory')\n",
    "    parser.add_argument('--epochs', type=int, default=DEFAULT_EPOCHS, help='Number of training epochs')\n",
    "    parser.add_argument('--batch_size', type=int, default=DEFAULT_BATCH_SIZE, help='Batch size')\n",
    "    parser.add_argument('--learning_rate', type=float, default=DEFAULT_LEARNING_RATE, help='Learning rate')\n",
    "    parser.add_argument('--no_cache', action='store_true', help='Skip using cached processed samples')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    main(args.data_dir, args.epochs, args.batch_size, args.learning_rate, args.no_cache)\n",
    "\n",
    "# For command line usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Check if we're in a Jupyter notebook\n",
    "    try:\n",
    "        # This will be True if running in Jupyter\n",
    "        get_ipython()\n",
    "        print(\"🔍 Detected Jupyter environment - running with default parameters\")\n",
    "        print(\"💡 To use custom parameters, call: main(epochs=5, batch_size=8, etc.)\")\n",
    "        main()  # Run with defaults in Jupyter\n",
    "    except NameError:\n",
    "        # We're in a regular Python script\n",
    "        main_with_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3bc0a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (gpu_ml)",
   "language": "python",
   "name": "gpu_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
